\documentclass[12pt]{diploma}
%\pdfpkresolution=8000

\begin{document}

	\begin{titlepage}
		\includepdf[pages=-,pagecommand={},scale=1]{title.pdf}
	\end{titlepage}
	
	\section*{Аннотация}
		
	Краткое описание диплома
		
	
	\tableofcontents 
	
	\newpage
		
	\section{Введение}
		В настоящее время наблюдается интенсивное развитие информационных технологий, появляются новые программные решения, применяемые в широком спектре областей. Если раньше информационные технологии были областью интересов узкого круга специалистов, то сейчас установилась тенденция к повсеместному распространению прикладных программных продуктов. 
		
		Современные компании вынуждены опираться на применение информационной инфраструктуры и использовать преимущества цифровых технологий для поддержания конкурентоспособности своих продуктов или услуг. В процессе эксплуатации информационных систем накапливаются массивы данных, обработка и интерпретация которых может принести компании коммерческую выгоду. 
		
		Каждый случай обработки данных, как правило, требует индивидуального подхода, не существует универсальной последовательности операций для любой задачи. Поэтому, обработка данных сопряжена с привлечением интеллектуальных усилий от высококвалифицированных специалистов. Программные системы анализа данных призваны облегчить этот труд и предоставляют в распоряжение специалиста наиболее востребованные процедуры обработки данных. Особенно актуально применение таких систем для решения прикладных задач, которые, несмотря на свою индивидуальность, зачастую однотипны и всё же имеют некоторые общие этапы решения. Применяя готовый, тщательно отлаженный и документированный программный код, сокращается время на анализ данных, а также снижается вероятность возникновения ошибок. 

		Прогресс технических средств в области сбора и обработки информации приводит к росту размеров массивов данных, которые требуется обрабатывать для удовлетворения потребностей компаний. Поэтому растёт роль методов агрегации данных и выделения в них общих закономерностей или структур. К таким методам, в частности относятся методы кластеризации. 
		
		Под кластеризацией понимают выделение объектов из таблицы наблюдений в множества, называемые кластерами, которые объединяют наиболее сходные объекты, при этом различные объекты должны попадать в разные кластеры \cite{mirkin-ds}. По принадлежности заданного нового объекта к определенному кластеру можно сделать предположения о его ключевых свойствах. Задачи кластеризации часто встают в самых разных областях, например, при обработке изображений или биологических структур а также социальных групп \cite{mirkin-clustering}.
		
		Вероятно, наиболее широко известный и популярный метод кластеризации --- \kmeans \cite{k-means-canonical}. Этот метод основан на поочерёдной минимизации квадратичного критерия по двум группам переменных: центрам кластеров и принадлежности объектов кластерам. На основе аналогичного квадратичного критерия предложены целый ряд эффективных алгоритмов, в том числе и иерархические, например, \Ward, \BisectingKmeans. 
		
		Однако, несмотря на популярность \kmeans, он обладает существенными недостатками. Первый заключается в том, что перед запуском алгоритма требуется знать число кластеров, которое должно быть выявлено. На практике, далеко не всегда число кластеров известно заранее. Например, в случае анализа интернет данных, сформированных из журналов посещений, и формирования групп схожих сайтов не предполагается наличие априорной информации. В таком случае придётся или рассматривать другие алгоритмы, или опираться на эмпирические зависимости.
		
		Второй недостаток заключается в том, что результат работы \kmeans сильно зависит от инициализации. В некоторых случаях неправильная инициализация может приводить к неудовлетворительным результатам. Этот недостаток обычно устраняют путём многократного запуска алгоритма для различных начальных условий, но при этом увеличивается и продолжительность кластеризации. Например, в случае обработки данных в режиме онлайн, такой способ может оказаться неприемлемым.  В то же время были предложены эффективные методы инициализации, которые позволяют оценить число кластеров с небольшими дополнительными временными затратами. 
		
		Третий существенный недостаток \kmeans состоит в снижении качества получаемого разбиения для случая зашумлённых данных. Многие данные, например составленные на основе измерений физических величин, содержат случайные погрешности, которые требуется учитывать при кластеризации для получения удовлетворительного результата. Отсутствие какого-либо механизма учёта шума в данных стимулирует исследователей предлагать усовершенствования \kmeans.
		
		Описанные выше недостатки \kmeans породили множество модификаций этого алгоритма, а также послужили импульсом для проведения работ над новыми алгоритмами, основанными на квадратичном критерии. Например, иерархический алгоритм \Ward \cite{ward-canonical} использует квадратичный критерий для агломеративного построения кластерной структуры. В совою очередь, \Ward также свойственны недостатки, которые частично были унаследованы от \kmeans, поэтому были предложены алгоритмы \Wardp и \AWardpb \cite{amorim-makarenkov-mirkin}, развивающие идею взвешенной кластеризации. Они продемонстрировали высокую эффективность при обработке зашумлённых данных, но, тем не менее, для канонической формулировки этих алгоритмов проблема выбора числа кластеров осталась неразрешенной. В дивизивных алгоритмах \BiKMR и \dePDDP \cite{kovaleva} используется автоматический критерий остановки, благодаря чему число кластеров определяется во время кластеризации.
		
		Таким образом, за последнее время появилось большое число новых и эффективных алгоритмов кластеризации, многие из них еще не реализованы в популярных библиотеках, таких как \texttt{scipy} для языка Python или  \texttt{Clustering Toolbox} для MATLAB. Авторы новых алгоритмов заявляют о их высокой эффективности и становится очевидно, что со временем эти алгоритмы найдут своё применение для задач определённой специфики. В данной работе рассматривается разработка программного обеспечения, в состав которого входит набор современных интеллектуальных алгоритмов кластеризации, основанных на критерии наименьших квадратов. Разработанная программа получила название ``Система интеллектуальной кластеризации данных'' (Intelligent Data Clustering Toolkit, INDACT).
		
		Программная система обладает графическим пользовательским интерфейсом, что делает её простой в применении даже для специалистов, не обладающих широкими познаниями в области программирования. В то же время пользовательский интерфейс представляет собой лишь надстройку над разработанной базовой библиотекой, которая включает в себя упомянутые алгоритмы. Эта библиотека имеет открытый код, снабжённый необходимой документацией, и тем самым допускает использование в других программных продуктах. 
		
	\section{Теоретическая часть}
	
	В теоретической части будут подробно описаны основные алгоритмы, реализованные в системе INDACT. Алгоритмы кластеризации по принципу работы разделяют на две категории: плоские и иерархические. К плоским относится, например, популярный \kmeans. Данная работа затрагивает в основном вторую категорию. Иерархические алгоритмы, в отличие от плоских, формируют вложенную структуру кластеров. Информация о взаимной вложенности кластеров может быть полезна для некоторых практических приложений, например, при исследовании биологических видов, вложенность кластеров может отражать филогенетическое родство. Это интересное свойство повышает интерес исследователей к иерархическим алгоритмам.
	
	Различают два вида иерархических алгоритмов: агломеративные (объединяющие) и дивизивные (разделяющие), которые соответственно реализуют восходящее и нисходящее направление формирования результирующего разбиения. Агломеративные алгоритмы рассматривают исходные данные как множество кластеров, состоящих из единственного объекта, который одновременно является центром. Итеративно происходит объединение двух ближайших кластеров, пока не будет выполнен заданный критерий останова. Дивизивные алгоритмы в противоположность агломеративным начинают работу с одного кластера, в который включены все объекты данных и разделяют его на более мелкие кластеры. 
	
	Программная система INDACT предоставляет пользователю выбор из четырёх современных алгоритмов кластеризации, два из которых агломеративные, а два --- дивизивные. Агломеративные алгоритмы \AWard и \AWardpb основаны на классическом критерии Ward, но расширяют традиционный подход с помощью предварительного этапа, называемого аномальной кластеризацией. Задача аномальной кластеризации состоит в предварительной разведке кластерной структуры и вычленении аномальных кластеров, которые расположены далеко от центра данных. Алгоритм \AWardpb, кроме того, предполагает взвешивание признаков, что может рассматриваться как механизм учёта случайных погрешностей. Два дивизивных алгоритма --- \dePDDP и \BiKMR используют автоматические критерии останова, выполнение которых проверяется на каждой итерации. Они хорошо подходят для тех случаев, когда число кластеров заранее неизвестно.
	
	Кроме кластеризации в состав программной системы включён модуль для генерации данных. Этот модуль позволяет сгенерировать при помощи небольшого числа управляющих параметров синтетические данные и на их примере проверить работу различных алгоритмов.

	\subsection{Постановка задачи кластеризации}
	В  работе используется представление данных в виде таблицы объект--признак. Такой формат данных широко распространён для практических приложений и встречается во многих реальных ситуациях. Программа INDACT не позволяет обрабатывать другие варианты представления данных. 
	
	Пусть имеется $ N $ объектов и у каждого объекта определены значения $ V $ признаков. Вообще говоря, признаки могут принимать как числовые значения, так и номинальные. Далее подразумевается, что если исходные данные имеют номинальные признаки, то их следует представить с помощью выбранного метода в виде числовых значений. Например, номинальный признак может быть разложен на несколько бинарных, каждый бинарный признак обозначает наличие (значение 1) или отсутствие (значение 0) соответствующего значения исходного признака.
	Множество всех объектов будем обозначать $ Y $. Тогда эти данные могут быть представлены в виде таблицы следующего вида:
	
	\begin{equation*}
	Y= \begin{pmatrix} 
	y_{1} \\
	\cdots \\ 
	y_{N} 
	\end{pmatrix}
	= \begin{pmatrix} 
	y_{11} & \cdots  & y_{1V} \\ 
	\cdots & \cdots  & \cdots \\ 
	y_{N1} & \cdots  & y_{NV} 
	\end{pmatrix}
	\end{equation*}
	
	Требуется получить разбиение $ S = \{C_1,\ldots,C_K\} $, состоящее из $ K $ кластеров, которые не пересекаются и покрывают всё множество объектов $ Y $. Кластер будем обозначать прописной буквой $ C $, а его центр --- строчной $ c $. В общем случае значение $ K $ не известно, хотя встречаются ситуации в которых число кластеров задано. Например, число кластеров может быть известно исходя из общих закономерностей предметной области. Чёткой формулировки относительно того, что должно быть включено в кластеры не существует. Общая идея состоит в том чтобы сходные объекты были включены в один кластер, а несходные не принадлежали одному кластеру. В схожесть объектов в различных приложениях может вкладываться различный математический смысл, например, схожесть объектов может определяться геометрической близостью или совпадением некоторых основных переменных.

	\subsection{Аномальная кластеризация} \label{subsec:anomalous}
	Аномальная кластеризация \cite{anomalous-clustring} имеет большое значение для всего программного комплекса, так как этот подход широко используется как составляющая часть более сложных алгоритмов, например, аномальная инициализация является первым шагом \AWard и \AWardpb. Этот этап позволяет предварительно ``разведать'' структуру данных и тем самым сформировать исходные предположения о возможном числе кластеров. 
	
	Алгоритмы, основанные на \kmeans требуют явного задания числа кластеров и начальных центров. Как правило, число кластеров определяется исходя из общих зависимостей и представлений пользователя о предметной области. Начальные центры кластеров определяются по результатам нескольких запусков для случайного положения с последующим выбором наилучшего результата. Зачастую у пользователя не имеется никаких представлений о предметной области относительно возможного числа кластеров. Поэтому в настоящее время идёт интенсивная работа над разработкой различных методов, которые позволяли бы определить число кластеров исходя исключительно из самих данных. 
	
	Аномальный кластер-анализ является одним из таких методов. Он основан на поочерёдном поиске аномальных групп и исключении их из данных, до тех пор, пока не останется ни одного объекта. Под аномальной группой понимается множество объектов, которые далеко отстоят от глобального центра данных.  Количество найденных аномальных групп объектов может служить приближением числа кластеров. Существуют алгоритмы, которые непосредственно используют найденное число кластеров в результате аномальной инициализации, например, \ikmeans \cite{anomalous-clustring}, а также алгоритмы, которые только опираются на этот результат для первой итерации и могут производить отличное число кластеров, например, \AWardpb.
	
	Метод аномальных кластеров можно рассматривать как модифицированный частный случай \kmeans при числе кластеров $ K=2 $. При этом инициализация начального положения центров кластеров жёстко определена --- один кластер всегда имеет центр в глобальном центре всех данных и не изменяется во время работы алгоритма, а центр второго кластера, который называется аномальным, инициализируется в наиболее удалённой от глобального центра точке. В процессе работы центр аномального кластера уточняется аналогично традиционному \kmeans. Отличие метода аномальных кластеров от \kmeans заключается в том, что один центр остаётся неизменным при на всех итерациях. На каждом шаге в аномальный кластер включаются те точки, которые лежат ближе к центру аномального кластера, чем к глобальному центру данных. После центр аномального кластера обновляется чтобы соответствовать среднему по всем включённым объектам. Когда центр аномального кластера стабилизировался и не изменяется, происходит исключение найденного кластера и продолжается работа с оставшимися данными. Исключение аномальных групп объектов происходит до тех пор, пока не останется ни одного объекта. 
	
	Формально алгоритм аномального кластер-анализа можно записать в следующем виде:
	
	\begin{algorithm}{Аномальная кластеризация (ik-means)}{ikmeans}	
%		\label{alg:ikmeans}
		\begin{astep}{Подготовка}
			Задаться пороговым значением минимальной численности аномального кластера $ \Theta $. Вычислить глобальный центр данных $ a = (a_1,\ldots,a_V) $:
			\begin{equation*}
				a_v=\frac{1}{N} \sum_{i=1}^{N}y_{iv}
			\end{equation*}
			Если имеются исходные представления о норме, использовать их для вычисления глобального центра. Стандартизировать таблицу данных сдвигом начала координат в точку $ a $.
		\end{astep}
		\begin{astep}{Инициализация аномального центра \label{itm:alg-anomal-init-center}}
			Определить наиболее удалённую от начала координат точку $ c $. Эта точка является начальным центром аномального кластера. 
		\end{astep}
		\begin{astep}{Обновление аномального кластера \label{itm:alg-anomal-update-cluster}}
			Объекты, которые расположены ближе к центру $ c $, чем к началу координат включить в аномальный кластер. Если в аномальный кластер не было внесено изменений, перейти к шагу  \ref{itm:alg-anomal-save-centroid}.
		\end{astep}
		\begin{astep}{Обновление центра \label{itm:alg-anomal-update-center}}
			Вычислить новый центр аномального кластера как покомпонентное среднее всех объектов, включённых в него.  Перейти к шагу \ref{itm:alg-anomal-update-cluster}.
		\end{astep}
		\begin{astep}{Сохранение центра \label{itm:alg-anomal-save-centroid}}
			Если число объектов в аномальном кластере больше заданного порогового значения $ \Theta $, сохранить центр и ассоциированный с ним аномальный кластер в список результатов.
		\end{astep}
		\begin{astep}{Исключение аномального кластера}
			Исключить из данных все объекты, которые принадлежат аномальному кластеру.  Если в таблице $ Y $ все ещё остаются объекты, прейти к шагу \ref{itm:alg-anomal-init-center}.
		\end{astep}
		\begin{astep}{Кластеризация}
			Выполнить алгоритм \kmeans \cite{k-means-canonical} на исходных данных $ Y $. При этом использовать центры аномальных кластеров, сохранённых на шаге \ref{itm:alg-anomal-save-centroid} в качестве начальных. Результатом работы \ikmeans является полученное разбиение.
		\end{astep}
	\end{algorithm}

	В \cite{mirkin-ds} показано, что аномальная кластеризация минимизирует критерий, подобный критерию метода \kmeans:
	\begin{equation*}
		W(S,c) = \sum_{i \in C}^{} d(y_i,c) + \sum_{i \notin C}^{} d(y_i,0),
	\end{equation*}
	\begin{conditions}
		 C     &  искомый кластер; \\
		 c     &  центр кластера $ С $; \\
		 d     &  квадрат евклидовского расстояния.\\  
	\end{conditions}

	\begin{figure} % \ContinuedFloat
		\centering
		\subfigure[Исходные данные] {\label{fig:ikmeans1}\includestandalone[width=0.49\textwidth]{tikz/v2/ik-means/ik-means-1}}
		\subfigure[Выбор начального центра аном. кластера] {\label{fig:ikmeans2}\includestandalone[width=0.49\textwidth]{tikz/v2/ik-means/ik-means-2}}
		\subfigure[Аномальный кластер сформирован] {\label{fig:ikmeans3}\includestandalone[width=0.49\textwidth]{tikz/v2/ik-means/ik-means-4}}
		\subfigure[Аномальный кластер исключён] {\label{fig:ikmeans4}\includestandalone[width=0.49\textwidth]{tikz/v2/ik-means/ik-means-5}}
		\subfigure[Итоговое разбиение аном. кластерами] {\label{fig:ikmeans5}\includestandalone[width=0.49\textwidth]{tikz/v2/ik-means/ik-means-22}}
		\subfigure[Результат после выполнения \kmeans] {\label{fig:ikmeans6}\includestandalone[width=0.49\textwidth]{tikz/v2/ik-means/ik-means-23}}
		\caption{\ikmeans: этапы выделения аномальных кластеров
			\\ {\small Каждому кластеру соответствуют точки определённого цвета в двумерном пространстве.}}
		\label{fig:ikmeans-working}
	\end{figure}

	Стоит заметить, что хотя аномальная кластеризация и позволяет предварительно ``разведать'' структуру данных, но на применение этого метода также имеются ограничения. Например, следует учитывать, что получаемая структура аномальных кластеров сгущается ближе к началу координат. Для сглаживания этого недостатка рекомендуется отбрасывать слишком маленькие аномальные группы, число объектов в которых меньше заданного значения $ \Theta $. 
	
	Рассмотрим работу алгоритма \ikmeans на простом примере. Пусть имеются двумерные данные, изображённые на рисунке \ref{fig:ikmeans1}. Первым шагом алгоритм определяет наиболее удалённую точку от глобального центра данных (на рисунке \ref{fig:ikmeans-working} обозначается чёрной звёздочкой) как показано на рисунке \ref{fig:ikmeans2}. Когда начальный центр аномального кластера определён, выполняются шаги некоторой вариации \kmeans при $ K=2 $ и жёстко зафиксированным центром данных. При этом центр аномального кластера уточняется до схождения (рисунок \ref{fig:ikmeans3}). Объекты, которые находятся ближе к центру аномального кластера, чем к глобальному центру исходных данных, включаются в состав аномального кластера и на после схождения очередной итерации, исключаются из данных без изменения глобального центра (см. рисунок \ref{fig:ikmeans4}). Когда все объекты будут исчерпаны, для полученных аномальных кластеров выполняется классическая версия \kmeans с числом кластеров, равным числу найденных аномальных групп и соответствующими центрами. Результат работы \ikmeans изображён на рисунке \ref{fig:ikmeans6}.	
	
	Метод аномальной кластеризации позволяет достичь хороших результатов, но как было показано в недавних исследованиях \cite{amorim-makarenkov-mirkin}, в случае задания стандартного порогового параметра $ \Theta=1 $, выделяет избыточное число кластеров. Определение рационального значения $ \Theta $ в настоящее время не имеет методической поддержки, поэтому для использования большого потенциала алгоритма было предложено его новое применение в роли предварительного этапа инициализации иерархических алгоритмов. 
		
	
	\subsection{Алгоритм \AWard}
	Алгоритм \AWard является модернизацией широко известного алгоритма иерархической кластеризации \Ward. Алгоритм \Ward \cite{ward-canonical} относится к агломеративным, то есть он действует по принципу ``снизу вверх''. На первом этапе принимается, что каждый отдельный объект $ y_i \in Y $ представляет собой кластер, центром которого является сам объект $ y_i $. Алгоритм \Ward итеративно выбирает два ближайших кластера и объединяет их, таким образом уменьшая общее число кластеров на единицу на каждой итерации. Процесс повторяется, пока не будет достигнуто заранее заданное число кластеров. Опишем последовательность шагов \Ward:
	\begin{algorithm}{Ward}{}	
		\begin{astep}{Инициализация}
			Задаться желаемым числом кластеров $ K^* $. Установить начальное число кластеров равным числу объектов $ K=N $. Исходное разбиение образованно элементарными кластерами $ S=\{C_1,\ldots,C_K\} $, в каждый из которых включён единственный объект $ y_i \in Y $ из исходных данных.
		\end{astep}
		\begin{astep}{Слияние кластеров \label{itm:ward-merge}}
			Выбрать два ближайших кластера $ C_a, C_b \in S $ согласно следующей формуле:
			\begin{equation}\label{eq:ward-dist}
				d_{Ward}(C_a,C_b) = \frac{N_1 N_2}{N_1 + N_2} \sum_{v=1}^{V} (c_{av}-c_{bv})^2,
			\end{equation}
			\begin{conditions}
				N_1,N_2       &  число объектов в кластерах $ C_a, C_b $; \\
				c_{av},c_{bv} &  $ v $-ая координата центра кластеров  $ C_a, C_b $. \\
			\end{conditions}
			
			Сформировать новый кластер $ C_{ab} $, в который входят все объекты из $ C_a $ и $ C_b $, удалив при этом старые кластеры $ C_a, C_b $.	Уменьшить текущее число кластеров $ K $ на 1. 
		\end{astep}
		\begin{astep}{Обновление центра}
			Вычислить новый центр сформированного кластера $ C_{ab} $ как покомпонентное среднее среди всех объектов этого кластера.
		\end{astep}
		\begin{astep}{Условие остановки}
			Если текущее число кластеров больше желаемого $ K>K^* $ и $ K>1 $, перейти к шагу \ref{itm:ward-merge}. В противном случае выдать текущее разбиение в качестве результата.
		\end{astep}
	\end{algorithm}

	Следует обратить внимание на два важных свойства алгоритма \Ward. Во-первых, как и любой иерархический алгоритм, \Ward формирует вложенную структуру классов, что может быть использовано в некоторых практических приложениях, где вложенность кластеров может быть естественным образом объяснена в терминах предметной области. Во-вторых, в канонической формулировке алгоритм не требует инициализации, как например, \kmeans. Таким образом, отпадает необходимость многократного запуска для определения наилучшего разбиения при различных начальных условиях.
	
	Но у \Ward есть существенный недостаток --- это продолжительность вычислений. В соответствии с описанной последовательностью шагов на каждой итерации алгоритма происходит вычисление расстояния между всеми кластерами. На первых итерациях число кластеров примерно равно числу объектов, это значит что требуется время, квадратично зависящее от $ N $. Этот недостаток послужил толчком для исследования возможности применения аномальной кластеризации для сокращения вычислений. 
	
	Из экспериментальных исследований известно, что аномальная кластеризация порождает избыточное число кластеров в то время как \Ward на каждом шаге сокращает число кластеров на единицу. Сочетание аномальной кластеризации и алгоритма \Ward позволит сократить время вычисления благодаря исключению стадии с большим числом маленьких кластеров. Такая модификация получила название \AWard \cite{amorim-makarenkov-mirkin}. 
	\begin{algorithm}{A-Ward}{}
		\begin{astep}{Инициализация \label{itm:a-ward-init}}
			Установить $ \Theta=1 $. Получить начальное число кластеров $ K $ и само разбиение  $ S $ по алгоритму \ikmeans (алгоритм  \ref{alg:ikmeans}).
		\end{astep}		
		\begin{astep}{Слияние кластеров \label{itm:a-ward-merge}}
			Выбрать два ближайших кластера $ C_a, C_b \in S $ согласно формуле \eqref{eq:ward-dist}.			
			Сформировать новый кластер $ C_{ab} $, в который входят все объекты из $ C_a $ и $ C_b $, удалив при этом старые кластеры $ C_a, C_b $.	Уменьшить текущее число кластеров $ K $ на 1. 
		\end{astep}
		\begin{astep}{Обновление центра}
			Вычислить новый центр сформированного кластера $ C_{ab} $ как покомпонентное среднее среди всех объектов этого кластера.
		\end{astep}
		\begin{astep}{Условие остановки}
			Если текущее число кластеров больше желаемого $ K>K^* $ и $ K>1 $, перейти к шагу \ref{itm:a-ward-merge}. В противном случае выдать текущее разбиение в качестве результата.
		\end{astep}
	\end{algorithm}
	
	На рисунке \ref{fig:ward-working} продемонстрирован принцип работы алгоритма \AWard для простых двумерных данных. Пусть после выполнения шага \ref{itm:a-ward-init} была получена кластерная структура, показанная на рисунке \ref{fig:ward1}. Исключительно в целях демонстрации зададим $ K^*=1 $, то есть алгоритм продолжит работать пока не будет сформирован единственных кластер из всех объектов. Предположим что, после сравнения всех расстояний между кластерами, вычисленных по формуле \eqref{eq:ward-dist} выяснилось, что красный и синий кластеры имеют наименьшее расстояние. Тогда они будут объединены в один кластер (для определённости присвоим ему красный цвет, хотя это не имеет значения). На следующем этапе будет произведено сравнение расстояний между оставшимися четырьмя кластерами и объединение ближайших, как показано на рисунке \ref{fig:ward2}. Объединение кластеров продолжится, пока два последних кластера не будут слиты в единый \ref{fig:ward5}. Схематично процесс слияния кластеров можно изобразить дендограммой \ref{fig:ward6}, которая показывает на каком этапе какие кластеры были объединены. Процесс иерархического кластер-анализа удобен с точки зрения остановки процесса: её можно произвести на любом этапе по заданному критерию, получив при этом соответствующее число кластеров. Таким образом, если было бы определено желаемое число кластеров, например, равное трём, процесс был бы остановлен через две итерации и результирующему разбиению соответствовала бы картинка \ref{fig:ward3}.
	
	Предложенное усовершенствование позволяет существенно сократить время работы алгоритма, благодаря исключению трудоёмких начальных стадий с большим числом кластеров. Вместо исходного элементарного разбиения используется предварительно найденные аномальные кластеры, число которые существенно меньше числа объектов.

	\begin{figure} % \ContinuedFloat
		\centering
		\subfigure[Первое объединение (синее и красное)] {\label{fig:ward1}\includestandalone[width=0.49\textwidth]{tikz/ward/ward1}}
		\subfigure[Второе объединение (зелёное и бирюзовое)] {\label{fig:ward2}\includestandalone[width=0.49\textwidth]{tikz/ward/ward2}}
		\subfigure[Третье объединение (зелёное и коричневое)] {\label{fig:ward3}\includestandalone[width=0.49\textwidth]{tikz/ward/ward3}}
		\subfigure[Четверное объединение (зеленое и красное)] {\label{fig:ward4}\includestandalone[width=0.49\textwidth]{tikz/ward/ward4}}
%		\subfigure[Процесс остановлен] {\label{fig:ward5}\includestandalone[width=0.49\textwidth]{tikz/ward/ward5}}
		\subfigure[Дерево вложенности кластеров] {\label{fig:ward6}\includestandalone[width=0.49\textwidth]{tikz/ward/ward-tree}}
		\caption{\AWard: этапы объединения кластеров, полученных в результате применения \ikmeans
			\\ {\small Каждому кластеру соответствуют точки определённого цвета в двумерном пространстве. Объединение кластеров обозначено серым затемнением с пунктирными границами.  На дендограмме цвет ветви соответствует цвету точек кластера.}}
		\label{fig:ward-working}
	\end{figure}
	
	\subsubsection{Специальный критерий остановки}
	В той формулировке алгоритма \AWard, которая была приведена выше не удаётся избавиться от традиционного недостатка, присущего многим алгоритмам, основанных на \kmeans. Этот недостаток заключается в необходимости задать желаемое число кластеров. Однако, в направлении разрешения этой проблемы был сделан вклад в работе \cite{spec-criterion}, в которой предложен специальный критерий остановки алгоритма. Предлагаемый способ завершения кластеризации в общем случае применим для иерархических алгоритмов с квадратичным критерием минимизации:
	\begin{equation} \label{eq:square-criterion}
		W(S,c) = \sum_{k=1}^{K} \sum_{i \in C_k}^{} \sum_{v=1}^{V} (y_{iv} - c_{kv})^2,
	\end{equation}
	\begin{conditions}
		W       & минимизируемый квадратичный критерий; \\
		S       & кластерное разбиение, $ S={C_1,\ldots,C_K} $; \\	
		K       & число кластеров, $ K = |S| $; \\
		C_k     & $ k $-ый кластер разбиения; \\
		c_{kv}  & $ v $-ая координата центра $ k $-го кластера; \\
		y_{iv}  & $ v $-ая координата центра $ i $-го объекта; \\
	\end{conditions}
	
	Пусть на некотором этапе работы происходит объединение некоторых двух кластеров $ C_k $ и $ C_l $ с формированием нового кластера $ C_{kl} $. Его центр можно вычислить по формуле: $ c_{kl} = (N_k \cdot c_k + N_l \cdot c_l)/(N_k + N_l) $, где $ N_k $ и $ N_l $ обозначают число объектов в кластерах $ C_k $ и $ C_l $ соответственно. При объединении кластеров происходит увеличение критерия \eqref{eq:square-criterion} на величину $ \Delta $:
	\begin{equation} \label{eq:spec-criterion-delta}
		\Delta(k,l) = W(S(k,l),c(k,l)) - W(S,c),
	\end{equation}
	\begin{conditions}
		S(k,l)       & разбиение, полученное из $ S $ разбиения после слияния кластеров $ C_k $ и $ C_l $; \\
		c(k,l)       & центры кластеров соответствующих разбиению $ S(k,l) $; \\
	\end{conditions}
	Величина $ \Delta $ всегда положительна, поскольку квадратичный критерий \eqref{eq:square-criterion} уменьшается с ростом числа кластеров и достигает нуля при $ K = N $. Величину изменения квадратичного критерия можно выразить через характеристики объединяемых кластеров:
	\begin{equation*}
		\Delta(k,l) = \frac{N_k N_l}{N_k + N_l}d(c_k,c_l)		
	\end{equation*}
	
	Как было отмечено в упомянутой статье \cite{spec-criterion}, при вычислении приращения квадратичного критерия \eqref{eq:square-criterion}  по формуле \eqref{eq:spec-criterion-delta} взаимно обнуляются все слагаемые, за исключением тех, которые относятся к кластерам $ C_k $, $ C_l $, $ C_{kl} =C_k \cup C_l $. Таким образом, можно переписать выражение \eqref{eq:spec-criterion-delta} следующим образом: $ 		\Delta(k,l) = w(C_k \cup C_l) - w(C_k) - w(C_l) $, где $  w(C) = \sum_{i\in C}^{} d(y_i,c) $ --- сумма квадратов евклидовских расстояний от точек кластера до его центра. 
	
	Предлагаемый специальный критерий имеет вид:
 	\begin{equation*}
	 	\Delta(k,l) < \alpha\:w(C_k \cup C_l)
 	\end{equation*}
 	где $ \alpha=\dfrac{1}{2} $.
	
	\subsection{Алгоритм \AWardpb}
		В реальный приложениях требуется анализировать данные с зашумлёнными признаками, которые были получены путём измерения некоторых физических параметров. В этом случае у алгоритмов \Ward и \AWard появляется неспособность отделения существенных признаков от шумовых. Для снижения влияния нерелевантных признаков предлагается ввести весовые коэффициенты, которые вычисляются на основании разброса значений признака: чем разброс больше, тем меньшую роль играет этот признак при кластеризации. 
		
		Модификация алгоритма \AWard, учитывающая вес признака получила название \AWardpb. В этой модификации также учтено обобщение для использования метрики Минковского произвольной степени. В обозначении алгоритма указано два параметра, буква $ p $ указывает на возможность изменения степени Минковского, а $ \beta $ --- степени весовых коэффициентов.
		
		Как и в случае с \AWard, для ускорения работы используется аномальная кластеризация. Алгоритм \AWardpb потребовал разработки обобщённой версии метода аномального кластер-анализа с использованием весовых коэффициентов и для заданной метрики Минковского. Ниже описана версия алгоритма \ref{alg:ikmeans} для использования с алгоритмом \AWardpb:
		
		\begin{algorithm}{Аномальная инициализация для $ A-Ward_{p\beta} $}{awardpb-init}
			\begin{astep}{Инициализация}
				Задаться значениями параметров $ p $ и $ \beta $. Вычислить глобальный центр данных $ c_Y $ как покомпонентный центр Минковского по всем объектам $ y_i \in Y $.
			\end{astep}
			\begin{astep}{Определение центра аномального кластера} \label{itm:awardpb-init-center}
				Аномальный кластер принять пустым, $ C_t = \varnothing $. Весовые коэффициенты распределить равномерно по всем признакам $ w_{kv} = 1/V $ при $ k=1,2 $ и $ v=1,\ldots,V $. За центр аномального кластера принять объект, который наиболее удалён от глобального центра $ c_Y $ по взвешенной метрике Минковского:
				\begin{equation} \label{eq:weighted-mink}
					d_{p\beta}(y_i, c_k) = \sum_{v=1}^{V}w^\beta_{kv}|y_{iv}-c_{kv}|^p
				\end{equation}
			\end{astep}
			\begin{astep}{Обновление аномального кластера}
				Каждый объект, который находится ближе к центру аномального кластера $ c_t $, чем к глобальному центру данных $ c_Y $, приписать к кластеру $ C_t $. Если при этом $ C_t $ остался неизменным, перейти к шагу \ref{itm:awardpb-init-save}.
			\end{astep}
			\begin{astep}{Обновление центра}
				Вычислить новый центр аномального кластера как покомпонентный центр Минковского по всем  объектам $ y_i \in C_t $.
			\end{astep}
			\begin{astep}{Обновление весовых коэффициентов}
				Вычислить новые весовые коэффициенты по формуле:
				\begin{eqnarray} \label{eq:weights}
					w_{kv}=\dfrac{1}{\sum_{u=1}^{V} \left ( \dfrac{D_{kv}}{D_{ku}} \right )^{\frac{1}{\beta-1}}},
				\end{eqnarray}	
				\begin{conditions}
					D_{kv} = \sum_{i\in C_k}{}|y_{iv} - c_{kv}|^\beta & разброс признака $ v $ в  кластере  $ C_k $. 
				\end{conditions}
			\end{astep}
			\begin{astep}{Сохранение параметров}\label{itm:awardpb-init-save}
				Включить текущий центр аномального кластера $ c_t $ в список центров $ \mathtt{c\_list} $, а веса $ w $ в список весов $ \mathtt{w\_list} $.
			\end{astep}
			\begin{astep}{Исключение аномального кластера}
				Исключить из $ Y $ все объекты $ y_i \in C_t $. Если $ Y \neq \varnothing $, перейти к шагу \ref{itm:awardpb-init-center}.
			\end{astep}
			\begin{astep}{Выдача результата}
				Результатом работы алгоритма является разбиение $ S $, состоящее из найденных аномальных кластеров, а также списки центров кластеров $ \mathtt{c\_list} $ и весов $ \mathtt{w\_list} $.
			\end{astep}
		\end{algorithm}
		
		Описанный алгоритм \ref{alg:awardpb-init} является первым этапом, выполняемым при аномальной инициализации \AWardpb. На следующем этапе происходит минимизация расстояния между объектами и центроидами с использованием ранее найденных центроидов $ с $ и весовых коэффициентов $ w $. Для этого применяется алгоритм \imwkmeanspb, который представляет собой модификацию \kmeans. Ниже описана последовательность шагов \imwkmeanspb:
		
		\begin{algorithm}{\mbox{imwk-means$ _{p\beta} $}}{imwkmeanspb}
			\begin{astep}{Инициализация}
				Установить текущее разбиение пустым $ S=\varnothing $, а число кластеров $ K $ равным длине списка $ \mathtt{c\_list} $, который был получен при аномальной инициализации (алгоритм  \ref{alg:awardpb-init}). 
			\end{astep}
			\begin{astep}{Формирование кластеров}\label{itm:imwkmeanspb-assignment}
				Каждый объект $ y_i \in Y $ поместить в кластер, центр которого $ c_k $ находится ближе всего к этому объекту. Близость объекта к центру кластера определяется  по формуле \eqref{eq:weighted-mink}. Если нет изменений в разбиении $  S $, перейти к шагу \ref{itm:imwkmeanspb-result}.
			\end{astep}
			\begin{astep}{Обновление центров} 
				Вычислить новые координаты центра $ c_k $ каждого кластера $ C_k $ как покомпонентный центр Минковского всех объектов этого кластера $ y_i \in C_k $.
			\end{astep}
			\begin{astep}{Обновление весов} 
				Вычислить новые веса $ w_{kv} $ по формуле \eqref{eq:weights} для $ k=1,\ldots,K $ и $ v=1,\ldots,V $. Перейти к шагу \ref{itm:imwkmeanspb-assignment}.
			\end{astep}
			\begin{astep}{Выдача результата} \label{itm:imwkmeanspb-result}
				Результатом работы алгоритма является разбиение $ S $, а также списки центров кластеров $ \mathtt{c\_list} $ и весов $ \mathtt{w\_list} $.
			\end{astep}			
		\end{algorithm}
		
		Третьим этапом работы \AWardpb является непосредственно иерархическая кластеризация, при которой число кластеров сокращается до необходимого количества. Полученное разбиение $ S $, а также центры кластеров и весовые коэффициенты используются для инициализации. Ниже приведён алгоритм \AWardpb.
		
		\begin{algorithm}{A-Ward$ _{p\beta}$}{}
			\begin{astep}{Инициализация} 
				Параметры $ p $ и $ \beta $ остаются неизменными, которые были определены для \mbox{$ imwk$-means$_{p\beta} $} (алгоритм \ref{alg:imwkmeanspb}). Начальное состояние соответствует конечному для \mbox{$ imwk$-means$_{p\beta} $}: исходный список центров кластеров $ \mathtt{c\_list} $ и весов $ \mathtt{w\_list} $ является результатом работы предыдущего этапа.
			\end{astep}
			\begin{astep}{Объединение кластеров}\label{itm:a-ward-p-beta-merge} 
				Выбрать два ближайших кластера $ C_a,\:C_b \in S$ и объединить их в новый $ C_{ab} $. Близость кластеров определяется по следующей формуле: 
				\begin{equation} \label{eq:ward-p-beta-distance}
				d_{Ward_{p\beta}}(C_a,C_b)=\dfrac{N_aN_b}{N_a+N_b}\sum_{v=1}^{V}\left ( \frac{w_{av}+w_{bv}}{2} \right )^\beta |c_{av}-c_{bv}|^p,
				\end{equation}				
				\begin{conditions}
					N_a,\:N_b  & количество объектов в кластерах $ C_a $ и $ C_b $ соответственно \\
					V          &  число признаков у каждого объекта $ y_i \in Y $ \\
					w_{av},\:w_{bv}  & веса $ v $-го признака в кластере $ C_a $ и $ C_b $ соответственно \\
					c_{av},\:c_{bv} & $ v $-ая координата центров кластеров $ C_a $ и $ C_b $ соответственно 
				\end{conditions}
			\end{astep}
			\begin{astep}{Обновление центра} 
				Вычислить новое значение центра $ C_{ab} $ как покомпонентный центр Минковского по всем объектам $ y_i \in C_{ab} $.
			\end{astep}
			\begin{astep}{Обновление весов} 
				Вычислить новые веса $ w_{kv} $ по формуле \eqref{eq:weights} для $ k=1,\ldots,K $ и $ v=1,\ldots,V $.				
			\end{astep}
			\begin{astep}{Условие остановки} 
				Уменьшить текущее число кластеров на единицу. Если текуще число кластеров все ещё больше единицы или требуемого числа кластеров, прейти к шагу \ref{itm:a-ward-p-beta-merge} .
			\end{astep}
		\end{algorithm}
		
		Алгоритм \AWardpb продемонстрировал свою эффективность как на искусственно сгенерированных данных, так и на реальных. Особый интерес предоставляют возможности алгоритма относительно восстановления кластеров на зашумлённых данных. Благодаря описанным нововведениям, алгоритм рассматривает значимость различных признаков с учётом дисперсии внутри кластера.
		
		Вопрос определения наиболее подходящих значений параметров $ p $ и $ \beta  $ на настоящее время изучен поверхностно. В частности, известно, что параметры должны задаваться для каждых данных индивидуально и могут сильно влиять на качество получаемого результата. Предложены базовые методы для нахождения рациональных значений при помощи перебора \cite{sw-based-search}. Указанная работа важна в контексте выбора критерия оценки качества разбиения без доступа к истинному разбиению. Авторы предлагают для этого использовать эмпирическую характеристику Silhouette Width (SW).
		
		В настоящее время ведутся исследования относительно возможного сокращения времени вычислений при подборе параметров. В анализе данных распространён метод кросс-валидации, идея которого заключается в том, что массив данных сохраняет все свои основные свойства и признаки при случайном исключении из него некоторого числа объектов. Та же идея лежит в основе предполагаемого метода ускоренного перебора параметров. Для сокращения времени одного выполнения алгоритма, осуществляется переход к небольшой подвыборке данных, сформированной случайным образом. Предполагается, что результат подбора параметров на подвыборке будет не сильно отличаться от подбора по полным данным с точки зрения качества результирующего разбиения, но при этом будет достигнута существенная экономия времени. Описанные исследования к настоящему моменту ещё не опубликованы, но если упомянутая гипотеза подтвердится, для алгоритма будут сформированы все предпосылки его применения для решения практических задач.
%		\subsubsection{Как найти минимум Минковского?}
	
	\subsection{Алгоритм \dePDDP}
	Алгоритм \dePDDP относится к иерархическим дивизивным и представляет собой усовершенствованную версию \PDDP (Principal Direction Divisive Partitioning) \cite{pddp-canonical}. Как и все дивизивные алгоритмы, \dePDDP начинает работу рассматривая все данные как единственный кластер, который включает в себя все объекты. Процесс продолжается до тех пор, пока не будет выполнен заданный критерий остановки.
	
	Итерационно происходит выбор одного кластера и его разбиение на два новых. Изначально в \PDDP критерий разделения был относительно простым: в один кластер выбирались те объекты, проекции которых на главную компоненту \cite{pca-canonical} лежали на положительной полуоси, а остальный объекты составляли второй кластер. Впоследствии авторами работы \cite{enhanced-pddp} этот критерий был пересмотрен для того, чтобы учесть распределение данных. Новое предложение заключалось в том, чтобы производить разбиение по наиболее глубокому минимуму функции плотности при проецировании данных на первую главную компоненту. 
	
	При оценке функции плотности используется метод ядерной оценки. Для заданного кластера $ C $ все признаки центрируются по объектам $ y_i \in C $, после чего определяется вектор главной компоненты как сингулярный вектор, соответствующий наибольшему сингулярному значению матрицы данных. Все объекты проецируются на ось главной компоненты и по формуле \eqref{eq:parzen} вычисляется оценка функции плотности Розенблата-Парзена:
	\begin{equation} \label{eq:parzen}
		\hat{f}(x_j) = \frac{1}{n\:h} \sum_{i=1}^{n} K((x_j - x_{n(i)})/h)
	\end{equation}
	\begin{conditions}
		n & число объектов в кластере \\
		x_{n(i)} & $ i $-ая точка в рассматриваемом кластере из $ n $ точек\\
		K(x) = \dfrac{1}{\sqrt{2\pi}}e^{-\dfrac{x^2}{2}}  & плотность нормального распределения \\
		h > 0 & параметр окна, определяемый по формуле: $ h= \sigma \left(\dfrac{4}{3n}\right)^{\dfrac{1}{5}} $, \newline
		где $ \sigma $ --- среднекв. отклонение проекций в кластере.
	\end{conditions}	

	Указанное нововведение, помимо основной цели, оказалось удачным одновременно для разрешения двух сопряжённых проблем: выбора очередного кластера для разбиения и остановки работы алгоритма. Было замечено, что минимум функции плотности может определять не только границу разбиения в пределах одного кластера, но и служить указателем на тот кластер, который должен быть разделен на текущей итерации. Таким образом, для разбиения выбирается кластер с наименьшим минимумом среди всех терминальных кластеров. 
	
	Если кластер имеет монотонную или выпуклую функцию плотности, то он не может быть разделен по данному критерию. Это наблюдение позволило определить естественный критерий останова алгоритма: алгоритм прекращает работу как только не останется ни одного кластера, который можно было бы разбить по критерию минимума плотности. Сформулируем алгоритм \dePDDP в виде последовательности шагов:

	\begin{algorithm}{dePDDP}{}
		\begin{astep}{Инициализация}
			Исходное число кластеров $ K $ принять равным одному. Создать новый кластер, который включает в себя все объекты данных.
		\end{astep}
		\begin{astep}{Оценка функции плотности} \label{itm:depddp-density-function}
			Для каждого нового кластера $ C_k $ в текущем разбиении найти главную компоненту по всем $ y_i \in C_k $ и спроецировать на неё все объекты кластера. Построить оценочную функцию плотности $ \hat{f_k}(x_i) $, руководствуясь формулой \eqref{eq:parzen}. Найти наиболее глубокий минимум функции плотности (если существует).
		\end{astep}
		\begin{astep}{Выбор кластера}
			Из текущего разбиения выбрать кластер $ C_{b}$ для разделения. Для этого найти кластер, у которого наиболее глубокий минимум достигает наименьшего значения среди всех кластеров. Если такого кластера нет, прейти к шагу \ref{itm:depddp-result}.
		\end{astep}
		\begin{astep}{Разделение}
			Разделить выбранный кластер $ C_{b} $. При этом создать два новых кластера, которые содержат объекты, лежащие соответственно по левую и правую сторону от минимума функции плотности при проецировании на ось главной компоненты. Исходный кластер $ C_{b} $ прекращает существование. Перейти к шагу \ref{itm:depddp-density-function}.
		\end{astep}
		\begin{astep}{Выдача результата} \label{itm:depddp-result}
			Результатом работы \dePDDP является текущее разбиение.
		\end{astep}
				
		
				
	\end{algorithm}
	
	Рассмотрим иллюстративно работу алгоритма \dePDDP на том же примере, что и \AWard (см. рисунок \ref{fig:depddp-working}). Данные состоят из пяти хорошо выделяемых кластеров. При этом на начальном этапе все данные принадлежат одному кластеру, который условно обозначим коричневым цветом, как показано на рисунке \ref{fig:depddp1}. Определим вектор главной компоненты (он направлен в том направлении, в котором разброс данных наибольший), который условно изображён в виде оси. Данные, спроецированные на ось главной компоненты, формируют оценку функции плотности с двумя минимумами, обозначенную на рисунке кривой линией. Выбирается наиболее глубокий минимум и осуществляется отделение синего кластера, как показано на рисунке \ref{fig:depddp2}. При этом оба софрмированных кластра имеют минимумы оценочной функции плотности. Выберем для разбиения тот, кластер, минимум которого глубже (допустим, это коричневый кластер). Очередное разбиение формирует кластер (коричневый на рисунке \ref{fig:depddp3}), который не имеет ни одного минимума при проецировании данных на главную компоненту, это значит что кластер сформирован окончательно и выбивает из рассмотрения. Разбиение кластеров продолжается до тех пор, пока не останется ни одного кластера, который можно было бы разбить (рисунок \ref{fig:depddp5}). Как только критерий остановы выполнился, алгоритмом возвращается результирующее разбиение, показанное на рисунке \ref{fig:depddp6}.
	
	Идея о разделении по минимуму функции плотности хорошо соответствует интуитивному понятию о кластерах, что было подтверждено в экспериментах с алгоритмом \dePDDP на синтетических и реальных данных \cite{enhanced-pddp,kovaleva}. 
	
	\begin{figure} % \ContinuedFloat
		\centering
		\subfigure[Первое раздвоение] {\label{fig:depddp1}\includestandalone[width=0.49\textwidth]{tikz/depddp/depddp1}}
		\subfigure[Второе раздвоение] {\label{fig:depddp2}\includestandalone[width=0.49\textwidth]{tikz/depddp/depddp2}}
		\subfigure[Третье раздвоение] {\label{fig:depddp3}\includestandalone[width=0.49\textwidth]{tikz/depddp/depddp3}}
		\subfigure[Четвёртое раздвоение] {\label{fig:depddp4}\includestandalone[width=0.49\textwidth]{tikz/depddp/depddp4}}
		\subfigure[Завершение работы] {\label{fig:depddp5}\includestandalone[width=0.49\textwidth]{tikz/depddp/depddp5}}
%		\subfigure[Результат] {\label{fig:depddp6}\includestandalone[width=0.49\textwidth]{tikz/depddp/depddp6}}
		\caption{\dePDDP: Этапы работы алгоритма
			\\ {\small Каждому кластеру соответствуют точки определённого цвета в двумерном пространстве. Осью показана первая главная компонента кластера. Чёрная кривая иллюстрирует оценку плотности данных, спроецированных на первую главную компоненту.}}
		\label{fig:depddp-working}
	\end{figure}
		
	\subsection{Алгоритм \BiKMR} \label{subsec:bikmr}
	Алгоритм \BiKMR (Bisecting \mbox{K-Means} with the Random projections stopping rule) \cite{kovaleva}, так же как и \dePDDP является иерархическим дивизивным. Среди дивизивных алгоритмов широкую известность получил простой алгоритм раздвоения по методу $ k $-средних (\BisectingKmeans) \cite{bisecting-k-means}, модификацией которого является \BiKMR. Алгоритм раздвоения по методу $ k $-средних использует квадратичный критерий для того чтобы разделить рассматриваемый кластер на два. Фактически этот подход является способом организации последовательного выполнения \kmeans (при $ K=2 $) для того чтобы получить иерархическую структуру кластеров. 
	
	Авторами \cite{kovaleva} были предложены следующие изменения в этот алгоритм: 
	\begin{itemize}
		\item использовать аномальные кластеры для инициализации \kmeans
		\item остановку осуществлять по критерию, учитывающему число минимумов оценочной функции плотности при проецировании объектов кластера на случайные направления
	\end{itemize}
	
	Использование аномальной кластеризации применяется для того чтобы избавиться от случайной инициализации \kmeans, применяемого для разделения кластера. Современные исследования относят необходимость инициализации к слабым сторонам \kmeans и утверждают о сильной зависимости результата кластеризации от правильного выбора исходных центров \cite{mirkin-core-concepts}. Благодаря применению метода аномальных кластеров, исходные центры определены однозначно и неизменно. Причём полученные центры способствуют высокому качеству результирующего разбиения и автоматически подбираются для данных \cite{mirkin-ds}. 
	
	Для дивизивных методов характерно большое разнообразие критериев остановки. Например, в классических случаях применяется остановка по достижению заданного числа кластеров. Для остановки \BiKMR  авторами предложен новый критерий, основанный на проецировании объектов кластеров на случайные направления. Пусть задано некоторое разбиение из $ K $ кластеров: $ S = \{C_1,\ldots,C_k\} $. Генерируется определённое число $ s $ случайных векторов $ p_t,\;t=1,\ldots,s $. Вектора предлагается генерировать сферическим распределением Гаусса с математическим ожиданием в начале координат и $ \sigma^2 = 1/V $, где $ V $ --- число признаков. Каждый объект $ y_i $ каждого кластера $ C_k $ спроецировать на все направления $ p_t $. Координата проекции вычисляется как скалярное произведение $ x_t = <x,p_t> $ после чего вычисляется функция плотности $ \hat{f_k^t} $ по методу ядерной оценки (формула \eqref{eq:parzen}). Для каждого кластера вычисляется отношение $ \epsilon_k $ числа функций $ \hat{f_k^t},\:t=1,\ldots,s $, которые имеют хотя бы один минимум к общему числу функций, то есть к $ s $. Если для некоторого кластера $ C_k $ отношение $ \epsilon_k $ меньше заданного пользователем порога $ \epsilon $ , то кластер $ C_k $ не разбивается. В данной работе рассматривается вариант алгоритма при котором, если кластер на очередном шаге не был разбит, он не будет рассматриваться как кандитат на следующих итерациях. Однако, вообще говоря, возможен случай, когда кластер будет разбит на следующих итерациях, так как направления генерируются каждый раз случайно. Если для всех кластеров верно условие остановки $ \epsilon_k<\epsilon $, то работа алгоритма прекращается. Авторы \cite{kovaleva} описывают также использование указанного критерия для непосредственного выполнения разбиения, то есть предлагается выбирать тот кластер $ C_k $ , у которого значение $ \epsilon_k $ наибольшее среди всех кластеров, при условии, что $ \epsilon_k > \epsilon $ и осуществлять разбиение по наиболее глубокому минимуму. В программе INDACT реализован метод разбиения с использованием \kmeans при $ K=2 $.
	
	Описанный критерий остановки требует указания двух параметров. Во-первых, это число случайных направлений, на которые проецируются объекты кластеров. Авторы алгоритма без подробных пояснений предлагают принять число направлений, равным числу признаков. Такое число с одной стороны обеспечит полноту критерия, с другой стороны будет не слишком затратным с точки зрения времени вычислений. Вопрос выбора значения $ \epsilon  $ проработан весьма подробно с  математическим обоснованием. Как оказалось, в среднем оправдан выбор значения $ \epsilon=0.32 $.
	
	Определим последовательность шагов для выполнения \BiKMR:
	\begin{algorithm}{BiKM-R}{}
		\begin{astep}{Инициализация}
			Задаться пороговым значением $ \epsilon $, как правило, $ \epsilon=0.32 $. 			Исходное число кластеров принять $ K=1 $. Создать новый кластер, который включает в себя все объекты данных.
		\end{astep}
		\begin{astep}{Генерация случайных направлений}
			Из сферического распределения Гаусса с нулевым математическим ожиданием и $ \sigma^2 = 1/V $ сгенерировать $ s=V $ случайных векторов. 
		\end{astep}
		\begin{astep}{Проецирование на случайные направления} \label{itm:bikmr-projecting}
			Все объекты каждого кластера спроецировать на сгенерированные направления. Вычислить функцию плотности $ \hat{f_k^t} $ для каждого направления $ t=1,\ldots,s $ по методу ядерной оценки (формула~\eqref{eq:parzen}). Для каждого кластера $ C_k $ найти долю направлений $ \epsilon_k $, функции плотности по которым имеют хотя бы один минимум. 
		\end{astep}
		\begin{astep}{Выбор кластера}
			Из текущего разбиения выбрать кластер $ C_{b} $ для которого доля $ \epsilon_b $ максимальна и больше порогового значения $ \epsilon $. Если такой кластер не найден, перейти к шагу \ref{itm:bikmr-result}.
		\end{astep}
		\begin{astep}{Разделение}
			Разделить кластер $ C_b $ с применением интеллектуального метода \ikmeans. Для того чтобы метод \ikmeans сгенерировал ровно два кластера, после аномальной инициализации выбрать два наибольших аномальных кластера в качестве центров \kmeans. После разделения исходный кластер $ C_b $ прекращает существование. Перейти к шагу \ref{itm:bikmr-projecting}.
		\end{astep}		
		\begin{astep}{Выдача результата} \label{itm:bikmr-result}
			Результатом работы \BiKMR является текущее разбиение.
		\end{astep}
	\end{algorithm}
	\subsection{Нормализация данных} \label{subsec:normalization}
		Нормализация данных играет значительную роль при применении алгоритмов кластеризации, основанных на критерии наименьших квадратов \cite{mirkin-ds}. Поэтому в программу INDACT включён модуль позволяющий применять различные способы масштабирования шкалы с сдвига начала координат. В данном разделе приведено  математическое описание возможных вариантов выполнения нормализации данных.
		\subsubsection{Общая формула нормализации данных}
		Пусть имеются данные заданные в табличном виде:
		
		\begin{equation}\label{eq:data-Y}
			Y= \begin{pmatrix} 
			y_{1} \\
			\cdots \\ 
			y_{N} 
			\end{pmatrix}
			= \begin{pmatrix} 
			y_{11} & \cdots  & y_{1V} \\ 
			\cdots & \cdots  & \cdots \\ 
			y_{N1} & \cdots  & y_{NV} 
			\end{pmatrix}
		\end{equation}
		
		Как правило, нормализацию данных проводят отдельно для каждого признака. Обозначим $ v $-ый столбец исходных данных в формуле \eqref{eq:data-Y} как $ Y_v $, тогда:
		\begin{equation*}
			Y=\left(Y_1,\ldots,Y_v,\ldots,Y_V\right)
		\end{equation*}
		
		Под нормализацией понимается преобразование исходных данных следующего вида:
		\begin{gather*}
			Y \rightarrow Y'=\left(Y_1',\ldots,Y_v',\ldots,Y_V'\right),\\
			Y_v'=\frac{Y_v-c_v}{r_v}
		\end{gather*}
		\begin{conditions}
			Y	    & исходные данные; \\
			Y'      & нормализованные данные;\\
			Y_v'	& нормализованный столбец данных, соответствующий исходному $ Y_v $;\\
			c_v = c(Y_v)     & величина, зависящая от $ Y_v $ и определяющая преобразование начала\newline отчёта;\\
			r_v = r(Y_v)    & величина, также зависящая от $ Y_v $ и определяющая преобразование \newline масштаба шкалы
		\end{conditions}
		\subsubsection{Преобразование начала отчёта}	
		Преобразование начала отчёта может быть произведено следующими способами:
		\begin{itemize}
			\item среднее $ c_v = \frac{1}{N}\sum_{i=1}^{N}y_{iv} $
			\item минимум $ c_v = \displaystyle \min_{i=1\ldots N}y_{iv} $
			\item медиана $ c_v = \underset{i=1\ldots N}{\mathrm{median}}\;y_{iv}$
			\item центр Минковского $ c_v = \underset{x}{\mathrm{arg\:min}} \sum_{i=1}^{N} |y_{iv} - x|^p $
		\end{itemize}
		
		В общем случае для вычисления центра Минковского при произвольном $ p $ не удаётся получить аналитическое решение. В программе INDACT используется итерационный алгоритм градиентного спуска.
	
		\subsubsection{Преобразование масштаба  шкалы}
		Для преобразования масштаба шкалы предусмотрено на выбор три варианта:
		\begin{itemize}
			\item полуразмах $\displaystyle r_v = \frac{1}{2}(\max_{i} y_{iv}- \min_{i}y_{iv}) $
			\item стандартное отклонение $ r_v = \sqrt{\frac{\sum_{i=1}^{N} (y_{iv}-\overline{y}_v)^2}{N-1}} $
			\item абсолютное отклонение $ r_v = \frac{1}{N}\sum_{i=1}^{N} | y_{iv} -  \underset{i=1\ldots N}{\mathrm{median}}\;y_{iv} | $
		\end{itemize}
	
	\subsection{Генератор синтетических данных}
	В программе INDACT предусмотрена возможность проведения численных экспериментов на синтетических данных. В данном разделе будет описан современный подход к генерации данных с использованием небольшого количества параметров, предложенный авторами статьи \cite{kovaleva}.
	
	Как правило, синтетические данные генерируются либо с хорошо различимыми кластерами \cite{data-gen1}, либо для генерации применяется сложный алгоритм, в котором настраивается множество различных параметров \cite{data-gen2}. Преимущества подхода, используемого в INDACT, заключаются в возможности одновременного регулирования разброса объектов внутри кластера и взаимное смешивание кластеров при помощи единственного параметра. 
	
	\begin{figure}[h!] % \ContinuedFloat
		\centering
		\subfigure[a=0.25] {\label{fig:svd-gen1}\includegraphics[width=0.49\textwidth]{img/svd-kovaleva-025}}
		\subfigure[a=0.50] {\label{fig:svd-gen2}\includegraphics[width=0.49\textwidth]{img/svd-kovaleva-050}}
		\subfigure[a=0.75] {\label{fig:svd-gen3}\includegraphics[width=0.49\textwidth]{img/svd-kovaleva-075}}
		
		\caption{Сгенерированные данные для трёх случаев параметра $ a $
			\\ {\small Для трёх значений параметра $ a = 0.25,\;0.50,\;0.75$ приведена диаграмма, изображающая проекции сгенерированных данных на главные компоненты. Данные сгенерированы при числе признаков, равным 10; общее число объектов --- 500; минимальная численность кластера --- 30.}}
		\label{fig:svd-gen}
	\end{figure}
	
	Структура получаемых данных представляет собой заданное количество гауссовых кластеров $ K $, сформированных при фиксированном общем числе объектов $ N $ и признаков $ V $ и заданным минимальным количестве объектов в каждом кластере $ m $. Остаток нераспределенных объектов $ \delta = N-K \cdot m $ размещается случайно и равномерно по всем кластерам. Для этого генерируется $ K-1 $ псевдослучайных числа в диапазоне от $ 0 $ до $ 1 $, к сгенерированным числам добавляется $ 0 $ и $ 1 $, после чего они упорядочиваются и вычисляется разность между соседними элементами в полученной последовательности. Эти разности определяют долю от нераспределенных объектов, которые будут дополнительно включены в $ K $ кластеров. Таким образом достигается строгое соблюдение общего числа объектов и равномерное распределение остатка по кластерам. 
	
	При определённой численности, кластер генерируется из многомерного распределения Гаусса. Среднее кластера выбирается случайно при равномерном распределении из множества $ [-a,a]^V $, где $ a \in [0,1] $ --- параметр, влияющий на взаимное смешивание кластеров: чем меньше $ a $, тем сложнее разделить сгенерированные кластеры. Ковариационная матрица формируется как диагональная с элементами, сгенерированными случайно из равномерного распределения в диапазоне $ [0.05\:a, 0.1\:a] $. 
	На рисунке показаны примеры сгенерированных данных 
	
	
	\subsection{Интерпретация результатов}
	
	В системе INDACT встроен модуль генерации отчётов, который позволяет пользователю получить основную информацию относительно каждого кластера и разбиения в целом. В данном разделе будут описаны математические зависимости, которые лежат в основе характеристик, используемых в отчёте. 
	
		
	\subsubsection{Оценка разбиений}
	
	При проведении кластер-анализа часто встаёт задача определения качества полученного разбиения. Например, вопрос о качестве разбиения может возникнуть при выборе наиболее подходящего алгоритма для анализируемых данных. В программе INDACT применяется две методики: индекс ARI и эмпирическая характеристика SW. Первая методика используется при сравнении разбиения с заданным эталонным. Эмпирическая характеристика SW не зависит от базового разбиения и может быть применена для одного разбиения при неизвестном эталонном. Однако, следует понимать, что SW не учитывает специфики предметной области и может в частных случаях давать искажённое представление о качестве разбиения. С другой стороны, на синтетических данных, для хорошо разделимых гауссовых кластеров, характеристика SW достаточно точно воспроизводит поведение индекса ARI относительно истинного разбиения, которое известно исходя из генерации кластеров.

	\subsubsection{Характеристика Silhouette Width}

	Эмпирическая характеристика Silhouette Width (SW) основывается на общем представлении о сильной близости внутри группы и хорошей разделимости кластеров. SW \cite{sw-canonical} принимает значения от -1 до 1 и для отдельного объекта вычисляется по следующей формуле:
	 \begin{equation*}
	 SW(y_i) = \dfrac{b(y_i)-a(y_i)}{max\{a(y_i),b(y_i))\}},
	 \end{equation*}
	 \begin{conditions}
	 	a(y_i) & среднее расстояние между объектом $ y_i \in C_k $ и всеми объектами,\newline     принадлежащими тому же кластеру $ C_k $, что и $ y_i $; \\
	 	b(y_i) & наименьшее среднее расстояние между объектом $ y_i \in C_k $ и объектами,\newline  которые принадлежат другим кластерам. \\
	 \end{conditions}
	 
	 Для разбиения характеристика SW определяется как среднее по всем объектам: $ SW(S) = \dfrac{1}{N} \sum_{k=1 }^{K} \sum_{i \in C_k}^{} SW(y_i) $. Значения, наиболее близкие к $ 1 $ соответствуют наилучшим разбиениям. Как показали эксперименты, характеристика SW ведёт себя примерно тем же образом что и широко известный индекс ARI \cite{ari-canonical}, который позволяет оценить качество разбиения, сравнивая его с истинным. 
	 
	
	\subsubsection{Adjusted Rand Index}
	
	Индекс ARI (Adjusted Rand Index) \cite{ari-canonical} является популярным способом сравнения эталонного и заданного разбиения. В условиях проводимого эксперимента используются синтетические данные, для которых известно истинное разбиение, поэтому для оценки эффективности применения эмпирической характеристики SW можно задействовать ARI. Формула для вычисления индекса записывается следующим образом:
	
	\begin{equation*} \label{eq:ari}
	ARI = \dfrac{\sum_{ij}\binom{n_{ij}}{2}-\left [ \sum_{i}\binom{a_{i}}{2}\sum_{j}\binom{b_{j}}{2} \right ]/\binom{n}{2}}{\dfrac{1}{2} \left [ \sum_i \binom{a_i}{2} + \sum_j \binom{b_j}{2} \right ] - \left [ \sum_i \binom{a_i}{2} \sum_j \binom{b_j}{2} \right ]/ \binom{n}{2}}
	\end{equation*}
	\begin{conditions}
		n_{ij} = |C_i \cap C_j|  & число объектов, входящий одновременно в $ i $-ый \newline  
		кластер в первом разбиении и в $ j $-ый --- во втором. \\
		a_{i} = \sum_{j=1}^{K} |C_i \cap C_j|  & число объектов, ходящих в $ i $-ый кластер в первом разбиении\\
		b_{j} = \sum_{i=1}^{K} |C_i \cap C_j|  & число объектов, ходящих в $ j $-ый кластер во втором разбиении\\
	\end{conditions}
	
	Как и характеристика SW, индекс ARI принимает значения от -1 до 1. ARI достигает 1 только в том случае, если два разбиения совпадают.
	
	\subsubsection{Характеристика кластеров}
	По каждому полученному кластеру $ C_k $ в отчёте INDACT приводится его центр, вычисленный для каждого признака как среднее по всем объектам кластера:
	
	\begin{equation*} 
	c_{kv} = \frac{1}{|C_k|} \sum_{i \in C_k}^{} y_{iv}
	\end{equation*}
	
	Центр кластера может вычисляться как по нормализованным, так и по исходным признакам. Для исходных признаков часто полезно знать разницу между глобальным центром данных и среднем по кластеру, как в абсолютном выражении, так и в процентном:
	
	\vspace*{1\baselineskip} 
	\noindent\begin{minipage}{.5\linewidth}
		\begin{equation*}
		D_{kv} = c_{kv} - \frac{1}{N} \sum_{i=1}^{N} y_{iv}
		\end{equation*}
	\end{minipage}%
	\begin{minipage}{.5\linewidth}
		\begin{equation*}
		D_{kv},\% = \frac{D_{kv}}{1/N \sum_{i=1}^{N} y_{iv}} \cdot 100 \% 
		\end{equation*}
	\end{minipage}%
	\vspace*{1\baselineskip} 

	Значимость кластера описывается его вкладом в квадратичный разброс данных. Квадратичный разброс данных и вклад в него заданного кластера $ C_k $ вычисляется по формулам, приведённым ниже:
	
	
	\vspace*{1\baselineskip} 
	\noindent\begin{minipage}{.5\linewidth}
		\begin{equation*}
		T(Y) = \sum_{i=1}^{N} y_{iv}^2
		\end{equation*}
	\end{minipage}%
	\begin{minipage}{.5\linewidth}
		\begin{equation*}
		contrib_k,\% =  |C_k| \frac{<c_{k},c_{k}> }{T(Y)}\cdot 100\%
		\end{equation*}
	\end{minipage}%
	\vspace*{1\baselineskip} 
	
	Общий вклад кластеров в квадратичный разброс данных может быть получен суммированием вклада по каждому из кластеров: $ contrib,\% = \sum_{k=1}^{K} contrib_k,\% $. Для того чтобы лучше понять что из себя представляют кластеры, в отчёте для каждого кластера отмечаются и выписываются признаки, которые превосходят по относительной разности заданный порог $ \Theta_D $. 	По-умолчанию пороговое значение выставлено равным $ 30\% $. Таким образом, в заданном кластере $ C_k $ признаки условно разделяются на:
	\begin{itemize}
		\item большие: $ \{v: D_{kv},\% > \Theta_D\} $
		\item маленькие: $ \{v: -D_{kv},\% > \Theta_D\} $
	\end{itemize}

	Вклад каждого кластера в квадратичный разброс данный может быть расписан по каждому признаку отдельно. Для определения вклада признака $ v $ в квадратичный разброс данных внутри кластера $ C_k $ применяется следующая формула:
	
	\begin{equation*}
	contrib_{kv},\% = |C_k| \frac{c_{kv}^2}{T(Y)} \cdot 100\%
	\end{equation*}	
	
	Сумма вклада $ contrib_{kv},\%  $ по всем признаком внутри кластера $ C_k $ равна общему вкладу этого кластера в разброс данных: $\sum_{v=1}^{V} contrib_{kv},\%  = contrib_{k},\%  $. Для вычисления относительного вклада вычисляется вклад каждого признака $ v $ без учёта разбиения $ contrib_v $ и определятся отношение вклада внутри кластера $ contrib_{kv},\%  $  к $ contrib_v $.
	
	\vspace*{1\baselineskip} 
	\noindent\begin{minipage}{.5\linewidth}
		\begin{equation*}
		contrib_v,\% = \frac{\sum_{i=1}^{N} y_{iv}^2}{T(Y)}			
		\end{equation*}
	\end{minipage}%
	\begin{minipage}{.5\linewidth}
		\begin{equation*}
		contrib_{kv}^{relative} = \frac{contrib_{kv}}{ contrib_v}
		\end{equation*}
	\end{minipage}%
	\vspace*{1\baselineskip} 
	
	\subsection{Общие рекомендации по выбору алгоритма}
	
	В разделах \ref{subsec:anomalous}---\ref{subsec:bikmr} описаны алгоритмы, реализованные программной системой INDACT. Полный  перечень алгоритмов, которые можно применить к данным, представлен ниже:
	
	\begin{enumerate}
		\item \ikmeans
		\item \AWard
		\item \AWardpb
		\item \dePDDP
		\item \BiKMR
	\end{enumerate}
	
	
	Алгоритмы \ikmeans, \dePDDP, \BiKMR находят число кластеров автоматически, причём по-умолчанию в них используются определённые значения параметров, выбор которых определяет число получаемых кластеров. В \ikmeans по-умолчанию задан порог $ \Theta=1 $ минимального числа объектов в аномальной группе, в алгоритме \dePDDP задействована величина ``окна Парзена'' $ h $, определяемая как указано в пояснениях к формуле \eqref{eq:parzen}, а в \BiKMR предопределена доля некорректных направлений $ \epsilon=0,32 $. Значения параметров в \dePDDP и \BiKMR найдены  экспериментально и, как правило, не требуют регулировки. Значение параметра в \ikmeans $ \Theta=1 $ обычно приводит к избыточному числу кластеров, что лежит в основе алгоритмов \AWard и \AWardpb, которые начинают именно с этих кластеров и последовательно их объединяют. При этом алгоритм \AWardpb включает два параметра, настраиваемых вручную. Его использование рекомендуется только в случае, когда ожидается, что признаки слабо соответствуют искомой кластерной структуре. Нормально, пользователю следует начать с алгоритма \ikmeans и применять \AWard для последовательного объединения полученных кластеров, если \ikmeans дал слишком большое число кластеров. Если ожидается, что в данных много случайных, нехарактерных объектов, то лучше применять метод \BiKMR.
	
	\section{Описание программы}
	
	С точки зрения пользователя программная система INDACT представляет собой оконное приложение с графическим интерфейсом, в целом аналогичным интерфейсу большинства современных программ. В данном разделе продемонстрированы основные элементы интерфейса а также некоторые диалоговые окна. Подробнее все функции системы описаны в руководстве пользователя.
	
	\subsection{Этапы работы с программой}
	
	Работа с программой в типичном случае состоит из четырёх базовых этапов, которые пользователь проходит последовательно  как изображено на рисунке \ref{fig:stages}. Тем не менее, это не означает что указанная последовательность жёстко фиксирована и нет возможности вернуться на предыдущий этап. Если пользователь допустил ошибку или намерен опробовать различные варианты, он может заново вызвать функции с произвольного предыдущего этапа.
	\begin{figure}[h!]
		\centering		
		\includestandalone[width=\linewidth]{tikz/stages}
		\caption{Основные этапы работы с программой}
		\label{fig:stages}
	\end{figure}
	
	На этапе загрузки данных пользователь должен выбрать текстовый файл с данными и загрузить его в программу. Текстовый файл представляет собой таблицу данных, каждая строка которой соответствует строке файла, а столбцы разделены запятыми. После загрузки файла графический интерфейс отобразит загруженные данные и они станут доступны для работы. Единовременно допускается работа с единственным файлом.
	
	Нормализация является подготовительным этапом, тем не менее, этот этап очень важен для успешной работы алгоритмов кластеризации. В диалоговом окне доступен выбор параметров нормализации описанных в разделе \ref{subsec:normalization}. Программа INDACT допускает изменение настроек нормализации практически на любом этапе обработки данных. В рамках нормализации в программной системе также рассматривается отбор признаков, которые будут участвовать в кластеризации.
	
	Этап кластеризации состоит в выборе подходящего алгоритма, задания параметров (при необходимости) и его выполнении. После выполнения алгоритма становится известно разбиение, которое ставит в соответствие каждому объекту номер кластера, которому объект принадлежит. Программа INDACT допускает работу одновременно с несколькими разбиениями. Таким образом, можно получить несколько различных результатов для разных значений параметров или алгоритмов и после этого приступить к анализу, выбрав наиболее подходящее разбиение. 
	
	Анализ результатов заключается в оценке полученных разбиений а также интерпретации кластеров. Для этого используется модуль генерации отчёта. Также функционалом системы предусмотрена возможность построения поля рассеивания по указанным признакам, а также гистограммы и SVD диаграммы.
	
	\subsection{Основные сведения о пользовательском интерфейсе}
	
	Главное окно программы INDACT содержит главное меню (1), управляющие кнопки (2) и две таблицы: исходных (3) и нормализованных данных (4) . Вид главного окна приведён на рисунке \ref{fig:main-gui}.

	\begin{figure}[h!]
		\centering
		\includestandalone[width=0.8\linewidth]{tikz/main-gui}				
		\caption{Главное окно программы INDACT}
		\label{fig:main-gui}
	\end{figure}
	
	Управляющие кнопки предназначены для следующих действий:
	\begin{itemize}
		\item \includegraphics[scale=0.05]{img/folder-ico} загрузить файл
		\item \includegraphics[scale=0.1]{img/settings} настройки нормализации
		\item \includegraphics[scale=0.05]{img/norm} вкл./выкл. нормализацию
		\item \includegraphics[scale=0.1]{img/norm_all} нормализовать выбранные признаки
		\item \includegraphics[scale=0.1]{img/delete} удалить выбранные признаки
		\item \includegraphics[scale=0.1]{img/clustering} отобразить/скрыть раздел результатов
	\end{itemize}
	
	Главное меню программы имеет структуру вложенных подменю как показано на рисунке \ref{fig:menu}.
	\begin{figure}[h!]
		\centering
		\includestandalone[width=\linewidth]{tikz/menu}				
		\caption{Структура главного меню}
		\label{fig:menu}
	\end{figure}
	
	Меню File содержит основные операции, такие как загрузка файла данных, операции с данными, сохранение данных и выход из программы. Операции с данными включают одновременную нормализацию нескольких признаков, очистку панели нормализованных данных и генерацию синтетических данных. 
	
	Пункт меню Run предоставляет пользователю возможность выбора одного из 5 реализованных алгоритмов. При выборе пункта меню, соответствующего алгоритму, при необходимости откроется окно ввода параметров алгоритма (некоторые алгоритмы, например, \dePDDP, не требуют настройки), через которое осуществляется запуск на выполнение.
	
	Меню Plot содержит базовые операции для графического представления данных. Например, у пользователя есть возможность присвоить признакам маркеры X и Y, определяющие признак, который будет отложен по оси абсцисс и ординат соответственно при построении поля рассеивания. Также пользователь может оценить общую структуру многомерных данных при помощи SVD диаграммы. 
	
	Для генерации отчёта предназначен пункт главного меню Report. Этот пункт содержит два действия для построения текстового отчёта и отчёта по центрам кластеров.
	
	
	
	\section{Структура программы и вычислений}
	\section{Демонстрационный пример}
	\section{Заключение}
%	\newpage
%	\subsection{Замечания и идеи}
%	\begin{enumerate}
%		\item
%		Все это дело написано на Pyhon, надо дать этому обоснование
%		\item
%		Не правильно описан алгоритм Ward. Должен быть цикл с предусловием. По факту происходит хотя бы 1 слияние, даже если желаемое число кластеров больше текущего.
%	\end{enumerate}
	
	
	
	
	
	\newpage
	
	\bibliography{bibliography}
	
\end{document}
